vocab_size: 50000
tokenizer_checkpoint: "tokenizer_checkpoint"
dataset_dir: "dataset"
seed: 123